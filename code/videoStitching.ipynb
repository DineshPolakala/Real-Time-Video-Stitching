{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install moviepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                          | 0/914 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: left.mp4 and right.mp4 loaded\n",
      "[INFO]: Video stitching starting....\n",
      "Frames =  914\n",
      "FPS = 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 914/914 [01:45<00:00,  8.70it/s]\n",
      "t:   0%|                                                                             | 0/915 [00:00<?, ?it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: Video stitching finished\n",
      "[INFO]: Saving home.mp4 in \n",
      "Moviepy - Building video home.mp4.\n",
      "Moviepy - Writing video home.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready home.mp4\n",
      "[INFO]: home.mp4 saved\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import imutils\n",
    "import tqdm\n",
    "import os\n",
    "from moviepy.editor import ImageSequenceClip\n",
    "\n",
    "\n",
    "class VideoStitcher:\n",
    "    def __init__(self, left_video_in_path, right_video_in_path, video_out_path, video_out_width=1500, display=False):\n",
    "        # Initialize arguments\n",
    "        self.left_video_in_path = left_video_in_path\n",
    "        self.right_video_in_path = right_video_in_path\n",
    "        self.video_out_path = video_out_path\n",
    "        self.video_out_width = video_out_width\n",
    "        self.display = display\n",
    "\n",
    "        # Initialize the saved homography matrix\n",
    "        self.saved_homo_matrix = None\n",
    "\n",
    "    def stitch(self, images, ratio=0.75, reproj_thresh=4.0):\n",
    "        # Unpack the images\n",
    "        (image_b, image_a) = images\n",
    "\n",
    "        # If the saved homography matrix is None, then we need to apply keypoint matching to construct it\n",
    "        if self.saved_homo_matrix is None:\n",
    "            # Detect keypoints and extract\n",
    "            (keypoints_a, features_a) = self.detect_and_extract(image_a)\n",
    "            (keypoints_b, features_b) = self.detect_and_extract(image_b)\n",
    "\n",
    "            # Match features between the two images\n",
    "            matched_keypoints = self.match_keypoints(keypoints_a, keypoints_b, features_a, features_b, ratio, reproj_thresh)\n",
    "\n",
    "            # If the match is None, then there aren't enough matched keypoints to create a panorama\n",
    "            if matched_keypoints is None:\n",
    "                return None\n",
    "\n",
    "            # Save the homography matrix\n",
    "            self.saved_homo_matrix = matched_keypoints[1]\n",
    "\n",
    "        # Apply a perspective transform to stitch the images together using the saved homography matrix\n",
    "        output_shape = (image_a.shape[1] + image_b.shape[1], image_a.shape[0])\n",
    "        result = cv2.warpPerspective(image_a, self.saved_homo_matrix, output_shape)\n",
    "        result[0:image_b.shape[0], 0:image_b.shape[1]] = image_b\n",
    "\n",
    "        # Return the stitched image\n",
    "        return result\n",
    "\n",
    "    @staticmethod\n",
    "    def detect_and_extract(image):\n",
    "        # Detect and extract features from the image (DoG keypoint detector and SIFT feature extractor)\n",
    "        #descriptor = cv2.xfeatures2d.SIFT_create()\n",
    "        descriptor = cv2.SIFT_create()\n",
    "        (keypoints, features) = descriptor.detectAndCompute(image, None)\n",
    "\n",
    "        # Convert the keypoints from KeyPoint objects to numpy arrays\n",
    "        keypoints = np.float32([keypoint.pt for keypoint in keypoints])\n",
    "\n",
    "        # Return a tuple of keypoints and features\n",
    "        return (keypoints, features)\n",
    "\n",
    "    @staticmethod\n",
    "    def match_keypoints(keypoints_a, keypoints_b, features_a, features_b, ratio, reproj_thresh):\n",
    "        # Compute the raw matches and initialize the list of actual matches\n",
    "        matcher = cv2.DescriptorMatcher_create(\"BruteForce\")\n",
    "        raw_matches = matcher.knnMatch(features_a, features_b, k=2)\n",
    "        matches = []\n",
    "\n",
    "        for raw_match in raw_matches:\n",
    "            # Ensure the distance is within a certain ratio of each other (i.e. Lowe's ratio test)\n",
    "            if len(raw_match) == 2 and raw_match[0].distance < raw_match[1].distance * ratio:\n",
    "                matches.append((raw_match[0].trainIdx, raw_match[0].queryIdx))\n",
    "\n",
    "        # Computing a homography requires at least 4 matches\n",
    "        if len(matches) > 4:\n",
    "            # Construct the two sets of points\n",
    "            points_a = np.float32([keypoints_a[i] for (_, i) in matches])\n",
    "            points_b = np.float32([keypoints_b[i] for (i, _) in matches])\n",
    "\n",
    "            # Compute the homography between the two sets of points\n",
    "            (homography_matrix, status) = cv2.findHomography(points_a, points_b, cv2.RANSAC, reproj_thresh)\n",
    "\n",
    "            # Return the matches, homography matrix and status of each matched point\n",
    "            return (matches, homography_matrix, status)\n",
    "\n",
    "        # No homography could be computed\n",
    "        return None\n",
    "\n",
    "    @staticmethod\n",
    "    def draw_matches(image_a, image_b, keypoints_a, keypoints_b, matches, status):\n",
    "        # Initialize the output visualization image\n",
    "        (height_a, width_a) = image_a.shape[:2]\n",
    "        (height_b, width_b) = image_b.shape[:2]\n",
    "        visualisation = np.zeros((max(height_a, height_b), width_a + width_b, 3), dtype=\"uint8\")\n",
    "        visualisation[0:height_a, 0:width_a] = image_a\n",
    "        visualisation[0:height_b, width_a:] = image_b\n",
    "\n",
    "        for ((train_index, query_index), s) in zip(matches, status):\n",
    "            # Only process the match if the keypoint was successfully matched\n",
    "            if s == 1:\n",
    "                # Draw the match\n",
    "                point_a = (int(keypoints_a[query_index][0]), int(keypoints_a[query_index][1]))\n",
    "                point_b = (int(keypoints_b[train_index][0]) + width_a, int(keypoints_b[train_index][1]))\n",
    "                cv2.line(visualisation, point_a, point_b, (0, 255, 0), 1)\n",
    "\n",
    "        # return the visualization\n",
    "        return visualisation\n",
    "\n",
    "    def run(self):\n",
    "        # Set up video capture\n",
    "        left_video = cv2.VideoCapture(self.left_video_in_path)\n",
    "        right_video = cv2.VideoCapture(self.right_video_in_path)\n",
    "        print('[INFO]: {} and {} loaded'.format(self.left_video_in_path.split('/')[-1],\n",
    "                                                self.right_video_in_path.split('/')[-1]))\n",
    "        print('[INFO]: Video stitching starting....')\n",
    "\n",
    "        # Get information about the videos\n",
    "        n_frames = min(int(left_video.get(cv2.CAP_PROP_FRAME_COUNT)),\n",
    "                       int(right_video.get(cv2.CAP_PROP_FRAME_COUNT)))\n",
    "        print(\"Frames = \",n_frames)\n",
    "        \n",
    "        fps = int(left_video.get(cv2.CAP_PROP_FPS))\n",
    "        \n",
    "        print(\"FPS = {}\".format(fps))\n",
    "        frames = []\n",
    "\n",
    "        for _ in tqdm.tqdm(np.arange(n_frames)):\n",
    "            # Grab the frames from their respective video streams\n",
    "            ok, left = left_video.read()\n",
    "            _, right = right_video.read()\n",
    "            #print(\"LEFT {}\".format(left))\n",
    "            #print(\"RIGHT {}\".format(right))\n",
    "\n",
    "            if ok:\n",
    "                # Stitch the frames together to form the panorama\n",
    "                stitched_frame = self.stitch([left, right])\n",
    "\n",
    "                # No homography could not be computed\n",
    "                if stitched_frame is None:\n",
    "                    print(\"[INFO]: Homography could not be computed!\")\n",
    "                    break\n",
    "\n",
    "                # Add frame to video\n",
    "                stitched_frame = imutils.resize(stitched_frame, width=self.video_out_width)\n",
    "                frames.append(stitched_frame)\n",
    "\n",
    "                if self.display:\n",
    "                    # Show the output images\n",
    "                    cv2.imshow(\"Result\", stitched_frame)\n",
    "\n",
    "                # If the 'q' key was pressed, break from the loop\n",
    "                if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "                    break\n",
    "\n",
    "        cv2.destroyAllWindows()\n",
    "        print('[INFO]: Video stitching finished')\n",
    "\n",
    "        # Save video\n",
    "        print('[INFO]: Saving {} in {}'.format(self.video_out_path.split('/')[-1],\n",
    "                                               os.path.dirname(self.video_out_path)))\n",
    "        clip = ImageSequenceClip(frames, fps=fps)\n",
    "        clip.write_videofile(self.video_out_path)\n",
    "        print('[INFO]: {} saved'.format(self.video_out_path.split('/')[-1]))\n",
    "        \n",
    "        \n",
    "#         path = \"home.mp4\"\n",
    "#         dinesh = cv2.VideoCapture(path)\n",
    "#         while True:\n",
    "            \n",
    "#             x,frame = dinesh.read()\n",
    "#             cv2.imshow(\"frame\",frame)\n",
    "#             if cv2.waitKey(1) and 0xff == ord(\"q\"):\n",
    "#                 break\n",
    "        \n",
    "        \"\"\"import numpy as np\n",
    "import cv2\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret,frame=cap.read()\n",
    "    #cv2.imshow(\"frame\",frame)\n",
    "    if cv2.waitKey(1)  and 0xff==ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\"\"\"\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "# Example call to 'VideoStitcher'\n",
    "stitcher = VideoStitcher(left_video_in_path='input/left.mp4',\n",
    "                         right_video_in_path='input/right.mp4',\n",
    "                         video_out_path='home.mp4')\n",
    "stitcher.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function write_videofile in module moviepy.video.VideoClip:\n",
      "\n",
      "write_videofile(self, filename, fps=None, codec=None, bitrate=None, audio=True, audio_fps=44100, preset='medium', audio_nbytes=4, audio_codec=None, audio_bitrate=None, audio_bufsize=2000, temp_audiofile=None, rewrite_audio=True, remove_temp=True, write_logfile=False, verbose=True, threads=None, ffmpeg_params=None, logger='bar')\n",
      "    Write the clip to a videofile.\n",
      "    \n",
      "    Parameters\n",
      "    -----------\n",
      "    \n",
      "    filename\n",
      "      Name of the video file to write in.\n",
      "      The extension must correspond to the \"codec\" used (see below),\n",
      "      or simply be '.avi' (which will work with any codec).\n",
      "    \n",
      "    fps\n",
      "      Number of frames per second in the resulting video file. If None is\n",
      "      provided, and the clip has an fps attribute, this fps will be used.\n",
      "    \n",
      "    codec\n",
      "      Codec to use for image encoding. Can be any codec supported\n",
      "      by ffmpeg. If the filename is has extension '.mp4', '.ogv', '.webm',\n",
      "      the codec will be set accordingly, but you can still set it if you\n",
      "      don't like the default. For other extensions, the output filename\n",
      "      must be set accordingly.\n",
      "    \n",
      "      Some examples of codecs are:\n",
      "    \n",
      "      ``'libx264'`` (default codec for file extension ``.mp4``)\n",
      "      makes well-compressed videos (quality tunable using 'bitrate').\n",
      "    \n",
      "    \n",
      "      ``'mpeg4'`` (other codec for extension ``.mp4``) can be an alternative\n",
      "      to ``'libx264'``, and produces higher quality videos by default.\n",
      "    \n",
      "    \n",
      "      ``'rawvideo'`` (use file extension ``.avi``) will produce\n",
      "      a video of perfect quality, of possibly very huge size.\n",
      "    \n",
      "    \n",
      "      ``png`` (use file extension ``.avi``) will produce a video\n",
      "      of perfect quality, of smaller size than with ``rawvideo``.\n",
      "    \n",
      "    \n",
      "      ``'libvorbis'`` (use file extension ``.ogv``) is a nice video\n",
      "      format, which is completely free/ open source. However not\n",
      "      everyone has the codecs installed by default on their machine.\n",
      "    \n",
      "    \n",
      "      ``'libvpx'`` (use file extension ``.webm``) is tiny a video\n",
      "      format well indicated for web videos (with HTML5). Open source.\n",
      "    \n",
      "    \n",
      "    audio\n",
      "      Either ``True``, ``False``, or a file name.\n",
      "      If ``True`` and the clip has an audio clip attached, this\n",
      "      audio clip will be incorporated as a soundtrack in the movie.\n",
      "      If ``audio`` is the name of an audio file, this audio file\n",
      "      will be incorporated as a soundtrack in the movie.\n",
      "    \n",
      "    audiofps\n",
      "      frame rate to use when generating the sound.\n",
      "    \n",
      "    temp_audiofile\n",
      "      the name of the temporary audiofile to be generated and\n",
      "      incorporated in the the movie, if any.\n",
      "    \n",
      "    audio_codec\n",
      "      Which audio codec should be used. Examples are 'libmp3lame'\n",
      "      for '.mp3', 'libvorbis' for 'ogg', 'libfdk_aac':'m4a',\n",
      "      'pcm_s16le' for 16-bit wav and 'pcm_s32le' for 32-bit wav.\n",
      "      Default is 'libmp3lame', unless the video extension is 'ogv'\n",
      "      or 'webm', at which case the default is 'libvorbis'.\n",
      "    \n",
      "    audio_bitrate\n",
      "      Audio bitrate, given as a string like '50k', '500k', '3000k'.\n",
      "      Will determine the size/quality of audio in the output file.\n",
      "      Note that it mainly an indicative goal, the bitrate won't\n",
      "      necessarily be the this in the final file.\n",
      "    \n",
      "    preset\n",
      "      Sets the time that FFMPEG will spend optimizing the compression.\n",
      "      Choices are: ultrafast, superfast, veryfast, faster, fast, medium,\n",
      "      slow, slower, veryslow, placebo. Note that this does not impact\n",
      "      the quality of the video, only the size of the video file. So\n",
      "      choose ultrafast when you are in a hurry and file size does not\n",
      "      matter.\n",
      "    \n",
      "    threads\n",
      "      Number of threads to use for ffmpeg. Can speed up the writing of\n",
      "      the video on multicore computers.\n",
      "    \n",
      "    ffmpeg_params\n",
      "      Any additional ffmpeg parameters you would like to pass, as a list\n",
      "      of terms, like ['-option1', 'value1', '-option2', 'value2'].\n",
      "    \n",
      "    write_logfile\n",
      "      If true, will write log files for the audio and the video.\n",
      "      These will be files ending with '.log' with the name of the\n",
      "      output file in them.\n",
      "    \n",
      "    logger\n",
      "      Either \"bar\" for progress bar or None or any Proglog logger.\n",
      "    \n",
      "    verbose (deprecated, kept for compatibility)\n",
      "      Formerly used for toggling messages on/off. Use logger=None now.\n",
      "    \n",
      "    Examples\n",
      "    ========\n",
      "    \n",
      "    >>> from moviepy.editor import VideoFileClip\n",
      "    >>> clip = VideoFileClip(\"myvideo.mp4\").subclip(100,120)\n",
      "    >>> clip.write_videofile(\"my_new_video.mp4\")\n",
      "    >>> clip.close()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(ImageSequenceClip.write_videofile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
